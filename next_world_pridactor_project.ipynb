{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ebr3p7y5LHGM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "data = \"\"\"Artificial intelligence is transforming the future of technology and society.\n",
        "Machine learning enables systems to improve their performance with experience.\n",
        "Deep learning is a subset of machine learning that uses multiple layers of neural networks.\n",
        "Recurrent neural networks are powerful for handling sequential data like text and speech.\n",
        "Long short term memory networks solve the problem of long range dependencies.\n",
        "The next word prediction model learns patterns from sequences of words.\n",
        "Training requires a large dataset of meaningful sentences and phrases.\n",
        "Text generation is a fascinating application of natural language processing.\n",
        "Neural networks learn to capture the probability of the next word in context.\n",
        "Optimization algorithms like Adam and SGD help minimize the training loss.\n",
        "Large amounts of text data improve the performance of predictive models.\n",
        "Evaluation metrics such as perplexity measure the quality of predictions.\n",
        "Language models can be applied to chatbots, translators, and text summarizers.\n",
        "Modern AI systems rely on deep architectures for state of the art performance.\n",
        "Data preprocessing is an important step in building robust models.\n",
        "Cleaning and tokenizing text ensures better learning during training.\n",
        "Vocabulary size determines the range of words that the model can predict.\n",
        "Embedding layers convert words into vector representations for the network.\n",
        "The sequence length defines how many previous words are used for prediction.\n",
        "Model training requires careful tuning of hyperparameters for accuracy.\n",
        "A validation dataset is used to monitor overfitting during training.\n",
        "Regularization techniques like dropout prevent the model from memorizing data.\n",
        "Batch size affects the stability and speed of gradient descent.\n",
        "Learning rate controls how quickly the model updates its weights.\n",
        "Too high a learning rate may cause the model to diverge during training.\n",
        "Too low a learning rate may slow down convergence of the model.\n",
        "Loss functions like categorical crossentropy are used for word prediction.\n",
        "The softmax function outputs the probability distribution for the next word.\n",
        "Predicted probabilities indicate the likelihood of different possible words.\n",
        "Beam search can be used for more accurate text generation.\n",
        "Greedy search simply chooses the most probable next word at each step.\n",
        "Sampling techniques introduce randomness into generated text.\n",
        "Language models capture both grammar and semantic meaning.\n",
        "Context plays a crucial role in predicting the correct next word.\n",
        "The quality of predictions improves with more diverse training data.\n",
        "Training can be done using GPUs to speed up computations.\n",
        "Evaluation helps determine if the model is generalizing well.\n",
        "Transfer learning allows the use of pre trained embeddings like GloVe or Word2Vec.\n",
        "Fine tuning can adapt pre trained models to specific datasets.\n",
        "Large models require more computation and memory to train effectively.\n",
        "Data augmentation strategies can enrich the training dataset.\n",
        "Generated text should be coherent and contextually relevant.\n",
        "Applications of next word prediction include mobile keyboards and assistants.\n",
        "Predictive typing saves time by suggesting the next word to users.\n",
        "Context aware models improve user experience in real world applications.\n",
        "Next word prediction is a step toward full sentence generation.\n",
        "Sequence to sequence models can handle tasks like translation and summarization.\n",
        "Attention mechanisms enhance the ability of models to focus on important words.\n",
        "Transformer models achieve remarkable results in natural language tasks.\n",
        "BERT and GPT are examples of powerful transformer architectures.\n",
        "These models are trained on billions of words from large corpora.\n",
        "Pre trained models can be fine tuned for specific downstream tasks.\n",
        "Natural language understanding is key to conversational AI systems.\n",
        "Speech recognition systems also benefit from language models.\n",
        "Autocomplete features rely heavily on next word prediction models.\n",
        "AI powered search engines use language models for query understanding.\n",
        "Personal assistants like Siri and Alexa use predictive language technologies.\n",
        "Chatbots employ language models to generate human like responses.\n",
        "Educational tools use AI to provide personalized learning experiences.\n",
        "Healthcare applications of AI include analyzing patient records and notes.\n",
        "Financial institutions use predictive models for fraud detection and analysis.\n",
        "Business intelligence systems rely on machine learning for decision making.\n",
        "Social media platforms use language models for content moderation.\n",
        "Recommendation systems use AI to suggest items based on user behavior.\n",
        "The ethics of AI requires careful consideration of fairness and bias.\n",
        "Responsible AI development ensures models do not harm individuals.\n",
        "Bias in training data can lead to unfair or inaccurate predictions.\n",
        "Transparency in AI helps build trust between humans and machines.\n",
        "Explainable AI is a growing field of research in machine learning.\n",
        "The interpretability of models allows humans to understand decisions.\n",
        "Open source libraries make AI development accessible to everyone.\n",
        "Frameworks like TensorFlow and PyTorch are widely used for deep learning.\n",
        "Keras provides a high level API for building neural networks quickly.\n",
        "Model deployment involves serving trained models to real users.\n",
        "APIs enable integration of AI models into applications and services.\n",
        "Scalability is important when serving models to millions of users.\n",
        "Cloud platforms provide infrastructure for training and deploying AI models.\n",
        "Edge devices allow AI models to run locally without internet.\n",
        "Model compression techniques reduce the size of large neural networks.\n",
        "Quantization and pruning are common strategies for efficient models.\n",
        "Energy efficient AI is an emerging research area for sustainability.\n",
        "Future AI systems may combine reasoning with statistical learning.\n",
        "Human and AI collaboration can enhance creativity and productivity.\n",
        "Education about AI is essential for preparing the future workforce.\n",
        "Ethical guidelines are necessary for safe and fair AI usage.\n",
        "The future of AI depends on responsible innovation and governance.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts([data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcSFrM8hLJYu",
        "outputId": "91a664d3-a1a0-4515-ed8a-976d9b200755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'the',\n",
              " 2: 'of',\n",
              " 3: 'and',\n",
              " 4: 'models',\n",
              " 5: 'to',\n",
              " 6: 'for',\n",
              " 7: 'ai',\n",
              " 8: 'learning',\n",
              " 9: 'is',\n",
              " 10: 'training',\n",
              " 11: 'a',\n",
              " 12: 'word',\n",
              " 13: 'model',\n",
              " 14: 'language',\n",
              " 15: 'can',\n",
              " 16: 'like',\n",
              " 17: 'next',\n",
              " 18: 'are',\n",
              " 19: 'text',\n",
              " 20: 'in',\n",
              " 21: 'systems',\n",
              " 22: 'data',\n",
              " 23: 'words',\n",
              " 24: 'on',\n",
              " 25: 'use',\n",
              " 26: 'networks',\n",
              " 27: 'prediction',\n",
              " 28: 'neural',\n",
              " 29: 'large',\n",
              " 30: 'be',\n",
              " 31: 'used',\n",
              " 32: 'trained',\n",
              " 33: 'future',\n",
              " 34: 'machine',\n",
              " 35: 'from',\n",
              " 36: 'predictive',\n",
              " 37: 'applications',\n",
              " 38: 'improve',\n",
              " 39: 'performance',\n",
              " 40: 'with',\n",
              " 41: 'deep',\n",
              " 42: 'requires',\n",
              " 43: 'dataset',\n",
              " 44: 'generation',\n",
              " 45: 'natural',\n",
              " 46: 'context',\n",
              " 47: 'predictions',\n",
              " 48: 'rely',\n",
              " 49: 'important',\n",
              " 50: 'step',\n",
              " 51: 'during',\n",
              " 52: 'size',\n",
              " 53: 'into',\n",
              " 54: 'sequence',\n",
              " 55: 'techniques',\n",
              " 56: 'rate',\n",
              " 57: 'may',\n",
              " 58: 'search',\n",
              " 59: 'more',\n",
              " 60: 'pre',\n",
              " 61: 'users',\n",
              " 62: 'tasks',\n",
              " 63: 'intelligence',\n",
              " 64: 'experience',\n",
              " 65: 'that',\n",
              " 66: 'layers',\n",
              " 67: 'powerful',\n",
              " 68: 'speech',\n",
              " 69: 'long',\n",
              " 70: 'memory',\n",
              " 71: 'range',\n",
              " 72: 'capture',\n",
              " 73: 'probability',\n",
              " 74: 'loss',\n",
              " 75: 'evaluation',\n",
              " 76: 'quality',\n",
              " 77: 'chatbots',\n",
              " 78: 'architectures',\n",
              " 79: 'an',\n",
              " 80: 'building',\n",
              " 81: 'ensures',\n",
              " 82: 'how',\n",
              " 83: 'careful',\n",
              " 84: 'tuning',\n",
              " 85: 'speed',\n",
              " 86: 'quickly',\n",
              " 87: 'too',\n",
              " 88: 'high',\n",
              " 89: 'generated',\n",
              " 90: 'helps',\n",
              " 91: 'allows',\n",
              " 92: 'or',\n",
              " 93: 'fine',\n",
              " 94: 'specific',\n",
              " 95: 'strategies',\n",
              " 96: 'include',\n",
              " 97: 'assistants',\n",
              " 98: 'user',\n",
              " 99: 'real',\n",
              " 100: 'enhance',\n",
              " 101: 'transformer',\n",
              " 102: 'understanding',\n",
              " 103: 'human',\n",
              " 104: 'provide',\n",
              " 105: 'platforms',\n",
              " 106: 'bias',\n",
              " 107: 'responsible',\n",
              " 108: 'development',\n",
              " 109: 'humans',\n",
              " 110: 'research',\n",
              " 111: 'serving',\n",
              " 112: 'efficient',\n",
              " 113: 'artificial',\n",
              " 114: 'transforming',\n",
              " 115: 'technology',\n",
              " 116: 'society',\n",
              " 117: 'enables',\n",
              " 118: 'their',\n",
              " 119: 'subset',\n",
              " 120: 'uses',\n",
              " 121: 'multiple',\n",
              " 122: 'recurrent',\n",
              " 123: 'handling',\n",
              " 124: 'sequential',\n",
              " 125: 'short',\n",
              " 126: 'term',\n",
              " 127: 'solve',\n",
              " 128: 'problem',\n",
              " 129: 'dependencies',\n",
              " 130: 'learns',\n",
              " 131: 'patterns',\n",
              " 132: 'sequences',\n",
              " 133: 'meaningful',\n",
              " 134: 'sentences',\n",
              " 135: 'phrases',\n",
              " 136: 'fascinating',\n",
              " 137: 'application',\n",
              " 138: 'processing',\n",
              " 139: 'learn',\n",
              " 140: 'optimization',\n",
              " 141: 'algorithms',\n",
              " 142: 'adam',\n",
              " 143: 'sgd',\n",
              " 144: 'help',\n",
              " 145: 'minimize',\n",
              " 146: 'amounts',\n",
              " 147: 'metrics',\n",
              " 148: 'such',\n",
              " 149: 'as',\n",
              " 150: 'perplexity',\n",
              " 151: 'measure',\n",
              " 152: 'applied',\n",
              " 153: 'translators',\n",
              " 154: 'summarizers',\n",
              " 155: 'modern',\n",
              " 156: 'state',\n",
              " 157: 'art',\n",
              " 158: 'preprocessing',\n",
              " 159: 'robust',\n",
              " 160: 'cleaning',\n",
              " 161: 'tokenizing',\n",
              " 162: 'better',\n",
              " 163: 'vocabulary',\n",
              " 164: 'determines',\n",
              " 165: 'predict',\n",
              " 166: 'embedding',\n",
              " 167: 'convert',\n",
              " 168: 'vector',\n",
              " 169: 'representations',\n",
              " 170: 'network',\n",
              " 171: 'length',\n",
              " 172: 'defines',\n",
              " 173: 'many',\n",
              " 174: 'previous',\n",
              " 175: 'hyperparameters',\n",
              " 176: 'accuracy',\n",
              " 177: 'validation',\n",
              " 178: 'monitor',\n",
              " 179: 'overfitting',\n",
              " 180: 'regularization',\n",
              " 181: 'dropout',\n",
              " 182: 'prevent',\n",
              " 183: 'memorizing',\n",
              " 184: 'batch',\n",
              " 185: 'affects',\n",
              " 186: 'stability',\n",
              " 187: 'gradient',\n",
              " 188: 'descent',\n",
              " 189: 'controls',\n",
              " 190: 'updates',\n",
              " 191: 'its',\n",
              " 192: 'weights',\n",
              " 193: 'cause',\n",
              " 194: 'diverge',\n",
              " 195: 'low',\n",
              " 196: 'slow',\n",
              " 197: 'down',\n",
              " 198: 'convergence',\n",
              " 199: 'functions',\n",
              " 200: 'categorical',\n",
              " 201: 'crossentropy',\n",
              " 202: 'softmax',\n",
              " 203: 'function',\n",
              " 204: 'outputs',\n",
              " 205: 'distribution',\n",
              " 206: 'predicted',\n",
              " 207: 'probabilities',\n",
              " 208: 'indicate',\n",
              " 209: 'likelihood',\n",
              " 210: 'different',\n",
              " 211: 'possible',\n",
              " 212: 'beam',\n",
              " 213: 'accurate',\n",
              " 214: 'greedy',\n",
              " 215: 'simply',\n",
              " 216: 'chooses',\n",
              " 217: 'most',\n",
              " 218: 'probable',\n",
              " 219: 'at',\n",
              " 220: 'each',\n",
              " 221: 'sampling',\n",
              " 222: 'introduce',\n",
              " 223: 'randomness',\n",
              " 224: 'both',\n",
              " 225: 'grammar',\n",
              " 226: 'semantic',\n",
              " 227: 'meaning',\n",
              " 228: 'plays',\n",
              " 229: 'crucial',\n",
              " 230: 'role',\n",
              " 231: 'predicting',\n",
              " 232: 'correct',\n",
              " 233: 'improves',\n",
              " 234: 'diverse',\n",
              " 235: 'done',\n",
              " 236: 'using',\n",
              " 237: 'gpus',\n",
              " 238: 'up',\n",
              " 239: 'computations',\n",
              " 240: 'determine',\n",
              " 241: 'if',\n",
              " 242: 'generalizing',\n",
              " 243: 'well',\n",
              " 244: 'transfer',\n",
              " 245: 'embeddings',\n",
              " 246: 'glove',\n",
              " 247: 'word2vec',\n",
              " 248: 'adapt',\n",
              " 249: 'datasets',\n",
              " 250: 'require',\n",
              " 251: 'computation',\n",
              " 252: 'train',\n",
              " 253: 'effectively',\n",
              " 254: 'augmentation',\n",
              " 255: 'enrich',\n",
              " 256: 'should',\n",
              " 257: 'coherent',\n",
              " 258: 'contextually',\n",
              " 259: 'relevant',\n",
              " 260: 'mobile',\n",
              " 261: 'keyboards',\n",
              " 262: 'typing',\n",
              " 263: 'saves',\n",
              " 264: 'time',\n",
              " 265: 'by',\n",
              " 266: 'suggesting',\n",
              " 267: 'aware',\n",
              " 268: 'world',\n",
              " 269: 'toward',\n",
              " 270: 'full',\n",
              " 271: 'sentence',\n",
              " 272: 'handle',\n",
              " 273: 'translation',\n",
              " 274: 'summarization',\n",
              " 275: 'attention',\n",
              " 276: 'mechanisms',\n",
              " 277: 'ability',\n",
              " 278: 'focus',\n",
              " 279: 'achieve',\n",
              " 280: 'remarkable',\n",
              " 281: 'results',\n",
              " 282: 'bert',\n",
              " 283: 'gpt',\n",
              " 284: 'examples',\n",
              " 285: 'these',\n",
              " 286: 'billions',\n",
              " 287: 'corpora',\n",
              " 288: 'tuned',\n",
              " 289: 'downstream',\n",
              " 290: 'key',\n",
              " 291: 'conversational',\n",
              " 292: 'recognition',\n",
              " 293: 'also',\n",
              " 294: 'benefit',\n",
              " 295: 'autocomplete',\n",
              " 296: 'features',\n",
              " 297: 'heavily',\n",
              " 298: 'powered',\n",
              " 299: 'engines',\n",
              " 300: 'query',\n",
              " 301: 'personal',\n",
              " 302: 'siri',\n",
              " 303: 'alexa',\n",
              " 304: 'technologies',\n",
              " 305: 'employ',\n",
              " 306: 'generate',\n",
              " 307: 'responses',\n",
              " 308: 'educational',\n",
              " 309: 'tools',\n",
              " 310: 'personalized',\n",
              " 311: 'experiences',\n",
              " 312: 'healthcare',\n",
              " 313: 'analyzing',\n",
              " 314: 'patient',\n",
              " 315: 'records',\n",
              " 316: 'notes',\n",
              " 317: 'financial',\n",
              " 318: 'institutions',\n",
              " 319: 'fraud',\n",
              " 320: 'detection',\n",
              " 321: 'analysis',\n",
              " 322: 'business',\n",
              " 323: 'decision',\n",
              " 324: 'making',\n",
              " 325: 'social',\n",
              " 326: 'media',\n",
              " 327: 'content',\n",
              " 328: 'moderation',\n",
              " 329: 'recommendation',\n",
              " 330: 'suggest',\n",
              " 331: 'items',\n",
              " 332: 'based',\n",
              " 333: 'behavior',\n",
              " 334: 'ethics',\n",
              " 335: 'consideration',\n",
              " 336: 'fairness',\n",
              " 337: 'do',\n",
              " 338: 'not',\n",
              " 339: 'harm',\n",
              " 340: 'individuals',\n",
              " 341: 'lead',\n",
              " 342: 'unfair',\n",
              " 343: 'inaccurate',\n",
              " 344: 'transparency',\n",
              " 345: 'build',\n",
              " 346: 'trust',\n",
              " 347: 'between',\n",
              " 348: 'machines',\n",
              " 349: 'explainable',\n",
              " 350: 'growing',\n",
              " 351: 'field',\n",
              " 352: 'interpretability',\n",
              " 353: 'understand',\n",
              " 354: 'decisions',\n",
              " 355: 'open',\n",
              " 356: 'source',\n",
              " 357: 'libraries',\n",
              " 358: 'make',\n",
              " 359: 'accessible',\n",
              " 360: 'everyone',\n",
              " 361: 'frameworks',\n",
              " 362: 'tensorflow',\n",
              " 363: 'pytorch',\n",
              " 364: 'widely',\n",
              " 365: 'keras',\n",
              " 366: 'provides',\n",
              " 367: 'level',\n",
              " 368: 'api',\n",
              " 369: 'deployment',\n",
              " 370: 'involves',\n",
              " 371: 'apis',\n",
              " 372: 'enable',\n",
              " 373: 'integration',\n",
              " 374: 'services',\n",
              " 375: 'scalability',\n",
              " 376: 'when',\n",
              " 377: 'millions',\n",
              " 378: 'cloud',\n",
              " 379: 'infrastructure',\n",
              " 380: 'deploying',\n",
              " 381: 'edge',\n",
              " 382: 'devices',\n",
              " 383: 'allow',\n",
              " 384: 'run',\n",
              " 385: 'locally',\n",
              " 386: 'without',\n",
              " 387: 'internet',\n",
              " 388: 'compression',\n",
              " 389: 'reduce',\n",
              " 390: 'quantization',\n",
              " 391: 'pruning',\n",
              " 392: 'common',\n",
              " 393: 'energy',\n",
              " 394: 'emerging',\n",
              " 395: 'area',\n",
              " 396: 'sustainability',\n",
              " 397: 'combine',\n",
              " 398: 'reasoning',\n",
              " 399: 'statistical',\n",
              " 400: 'collaboration',\n",
              " 401: 'creativity',\n",
              " 402: 'productivity',\n",
              " 403: 'education',\n",
              " 404: 'about',\n",
              " 405: 'essential',\n",
              " 406: 'preparing',\n",
              " 407: 'workforce',\n",
              " 408: 'ethical',\n",
              " 409: 'guidelines',\n",
              " 410: 'necessary',\n",
              " 411: 'safe',\n",
              " 412: 'fair',\n",
              " 413: 'usage',\n",
              " 414: 'depends',\n",
              " 415: 'innovation',\n",
              " 416: 'governance'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O22zjmhINKxB"
      },
      "outputs": [],
      "source": [
        "input_sequannce=[]\n",
        "for sentence in data.split('\\n'):\n",
        "  tokanized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for i in range(1, len(tokanized_sentence)):\n",
        "    input_sequannce.append(tokanized_sentence[:i+1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoZaak1hQz2g",
        "outputId": "5d17376c-0ee0-493b-f4e7-4a7d1dd323c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[113, 63],\n",
              " [113, 63, 9],\n",
              " [113, 63, 9, 114],\n",
              " [113, 63, 9, 114, 1],\n",
              " [113, 63, 9, 114, 1, 33],\n",
              " [113, 63, 9, 114, 1, 33, 2],\n",
              " [113, 63, 9, 114, 1, 33, 2, 115],\n",
              " [113, 63, 9, 114, 1, 33, 2, 115, 3],\n",
              " [113, 63, 9, 114, 1, 33, 2, 115, 3, 116],\n",
              " [34, 8],\n",
              " [34, 8, 117],\n",
              " [34, 8, 117, 21],\n",
              " [34, 8, 117, 21, 5],\n",
              " [34, 8, 117, 21, 5, 38],\n",
              " [34, 8, 117, 21, 5, 38, 118],\n",
              " [34, 8, 117, 21, 5, 38, 118, 39],\n",
              " [34, 8, 117, 21, 5, 38, 118, 39, 40],\n",
              " [34, 8, 117, 21, 5, 38, 118, 39, 40, 64],\n",
              " [41, 8],\n",
              " [41, 8, 9],\n",
              " [41, 8, 9, 11],\n",
              " [41, 8, 9, 11, 119],\n",
              " [41, 8, 9, 11, 119, 2],\n",
              " [41, 8, 9, 11, 119, 2, 34],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120, 121],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120, 121, 66],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120, 121, 66, 2],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120, 121, 66, 2, 28],\n",
              " [41, 8, 9, 11, 119, 2, 34, 8, 65, 120, 121, 66, 2, 28, 26],\n",
              " [122, 28],\n",
              " [122, 28, 26],\n",
              " [122, 28, 26, 18],\n",
              " [122, 28, 26, 18, 67],\n",
              " [122, 28, 26, 18, 67, 6],\n",
              " [122, 28, 26, 18, 67, 6, 123],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124, 22],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124, 22, 16],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124, 22, 16, 19],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124, 22, 16, 19, 3],\n",
              " [122, 28, 26, 18, 67, 6, 123, 124, 22, 16, 19, 3, 68],\n",
              " [69, 125],\n",
              " [69, 125, 126],\n",
              " [69, 125, 126, 70],\n",
              " [69, 125, 126, 70, 26],\n",
              " [69, 125, 126, 70, 26, 127],\n",
              " [69, 125, 126, 70, 26, 127, 1],\n",
              " [69, 125, 126, 70, 26, 127, 1, 128],\n",
              " [69, 125, 126, 70, 26, 127, 1, 128, 2],\n",
              " [69, 125, 126, 70, 26, 127, 1, 128, 2, 69],\n",
              " [69, 125, 126, 70, 26, 127, 1, 128, 2, 69, 71],\n",
              " [69, 125, 126, 70, 26, 127, 1, 128, 2, 69, 71, 129],\n",
              " [1, 17],\n",
              " [1, 17, 12],\n",
              " [1, 17, 12, 27],\n",
              " [1, 17, 12, 27, 13],\n",
              " [1, 17, 12, 27, 13, 130],\n",
              " [1, 17, 12, 27, 13, 130, 131],\n",
              " [1, 17, 12, 27, 13, 130, 131, 35],\n",
              " [1, 17, 12, 27, 13, 130, 131, 35, 132],\n",
              " [1, 17, 12, 27, 13, 130, 131, 35, 132, 2],\n",
              " [1, 17, 12, 27, 13, 130, 131, 35, 132, 2, 23],\n",
              " [10, 42],\n",
              " [10, 42, 11],\n",
              " [10, 42, 11, 29],\n",
              " [10, 42, 11, 29, 43],\n",
              " [10, 42, 11, 29, 43, 2],\n",
              " [10, 42, 11, 29, 43, 2, 133],\n",
              " [10, 42, 11, 29, 43, 2, 133, 134],\n",
              " [10, 42, 11, 29, 43, 2, 133, 134, 3],\n",
              " [10, 42, 11, 29, 43, 2, 133, 134, 3, 135],\n",
              " [19, 44],\n",
              " [19, 44, 9],\n",
              " [19, 44, 9, 11],\n",
              " [19, 44, 9, 11, 136],\n",
              " [19, 44, 9, 11, 136, 137],\n",
              " [19, 44, 9, 11, 136, 137, 2],\n",
              " [19, 44, 9, 11, 136, 137, 2, 45],\n",
              " [19, 44, 9, 11, 136, 137, 2, 45, 14],\n",
              " [19, 44, 9, 11, 136, 137, 2, 45, 14, 138],\n",
              " [28, 26],\n",
              " [28, 26, 139],\n",
              " [28, 26, 139, 5],\n",
              " [28, 26, 139, 5, 72],\n",
              " [28, 26, 139, 5, 72, 1],\n",
              " [28, 26, 139, 5, 72, 1, 73],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2, 1],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2, 1, 17],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2, 1, 17, 12],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2, 1, 17, 12, 20],\n",
              " [28, 26, 139, 5, 72, 1, 73, 2, 1, 17, 12, 20, 46],\n",
              " [140, 141],\n",
              " [140, 141, 16],\n",
              " [140, 141, 16, 142],\n",
              " [140, 141, 16, 142, 3],\n",
              " [140, 141, 16, 142, 3, 143],\n",
              " [140, 141, 16, 142, 3, 143, 144],\n",
              " [140, 141, 16, 142, 3, 143, 144, 145],\n",
              " [140, 141, 16, 142, 3, 143, 144, 145, 1],\n",
              " [140, 141, 16, 142, 3, 143, 144, 145, 1, 10],\n",
              " [140, 141, 16, 142, 3, 143, 144, 145, 1, 10, 74],\n",
              " [29, 146],\n",
              " [29, 146, 2],\n",
              " [29, 146, 2, 19],\n",
              " [29, 146, 2, 19, 22],\n",
              " [29, 146, 2, 19, 22, 38],\n",
              " [29, 146, 2, 19, 22, 38, 1],\n",
              " [29, 146, 2, 19, 22, 38, 1, 39],\n",
              " [29, 146, 2, 19, 22, 38, 1, 39, 2],\n",
              " [29, 146, 2, 19, 22, 38, 1, 39, 2, 36],\n",
              " [29, 146, 2, 19, 22, 38, 1, 39, 2, 36, 4],\n",
              " [75, 147],\n",
              " [75, 147, 148],\n",
              " [75, 147, 148, 149],\n",
              " [75, 147, 148, 149, 150],\n",
              " [75, 147, 148, 149, 150, 151],\n",
              " [75, 147, 148, 149, 150, 151, 1],\n",
              " [75, 147, 148, 149, 150, 151, 1, 76],\n",
              " [75, 147, 148, 149, 150, 151, 1, 76, 2],\n",
              " [75, 147, 148, 149, 150, 151, 1, 76, 2, 47],\n",
              " [14, 4],\n",
              " [14, 4, 15],\n",
              " [14, 4, 15, 30],\n",
              " [14, 4, 15, 30, 152],\n",
              " [14, 4, 15, 30, 152, 5],\n",
              " [14, 4, 15, 30, 152, 5, 77],\n",
              " [14, 4, 15, 30, 152, 5, 77, 153],\n",
              " [14, 4, 15, 30, 152, 5, 77, 153, 3],\n",
              " [14, 4, 15, 30, 152, 5, 77, 153, 3, 19],\n",
              " [14, 4, 15, 30, 152, 5, 77, 153, 3, 19, 154],\n",
              " [155, 7],\n",
              " [155, 7, 21],\n",
              " [155, 7, 21, 48],\n",
              " [155, 7, 21, 48, 24],\n",
              " [155, 7, 21, 48, 24, 41],\n",
              " [155, 7, 21, 48, 24, 41, 78],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6, 156],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6, 156, 2],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6, 156, 2, 1],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6, 156, 2, 1, 157],\n",
              " [155, 7, 21, 48, 24, 41, 78, 6, 156, 2, 1, 157, 39],\n",
              " [22, 158],\n",
              " [22, 158, 9],\n",
              " [22, 158, 9, 79],\n",
              " [22, 158, 9, 79, 49],\n",
              " [22, 158, 9, 79, 49, 50],\n",
              " [22, 158, 9, 79, 49, 50, 20],\n",
              " [22, 158, 9, 79, 49, 50, 20, 80],\n",
              " [22, 158, 9, 79, 49, 50, 20, 80, 159],\n",
              " [22, 158, 9, 79, 49, 50, 20, 80, 159, 4],\n",
              " [160, 3],\n",
              " [160, 3, 161],\n",
              " [160, 3, 161, 19],\n",
              " [160, 3, 161, 19, 81],\n",
              " [160, 3, 161, 19, 81, 162],\n",
              " [160, 3, 161, 19, 81, 162, 8],\n",
              " [160, 3, 161, 19, 81, 162, 8, 51],\n",
              " [160, 3, 161, 19, 81, 162, 8, 51, 10],\n",
              " [163, 52],\n",
              " [163, 52, 164],\n",
              " [163, 52, 164, 1],\n",
              " [163, 52, 164, 1, 71],\n",
              " [163, 52, 164, 1, 71, 2],\n",
              " [163, 52, 164, 1, 71, 2, 23],\n",
              " [163, 52, 164, 1, 71, 2, 23, 65],\n",
              " [163, 52, 164, 1, 71, 2, 23, 65, 1],\n",
              " [163, 52, 164, 1, 71, 2, 23, 65, 1, 13],\n",
              " [163, 52, 164, 1, 71, 2, 23, 65, 1, 13, 15],\n",
              " [163, 52, 164, 1, 71, 2, 23, 65, 1, 13, 15, 165],\n",
              " [166, 66],\n",
              " [166, 66, 167],\n",
              " [166, 66, 167, 23],\n",
              " [166, 66, 167, 23, 53],\n",
              " [166, 66, 167, 23, 53, 168],\n",
              " [166, 66, 167, 23, 53, 168, 169],\n",
              " [166, 66, 167, 23, 53, 168, 169, 6],\n",
              " [166, 66, 167, 23, 53, 168, 169, 6, 1],\n",
              " [166, 66, 167, 23, 53, 168, 169, 6, 1, 170],\n",
              " [1, 54],\n",
              " [1, 54, 171],\n",
              " [1, 54, 171, 172],\n",
              " [1, 54, 171, 172, 82],\n",
              " [1, 54, 171, 172, 82, 173],\n",
              " [1, 54, 171, 172, 82, 173, 174],\n",
              " [1, 54, 171, 172, 82, 173, 174, 23],\n",
              " [1, 54, 171, 172, 82, 173, 174, 23, 18],\n",
              " [1, 54, 171, 172, 82, 173, 174, 23, 18, 31],\n",
              " [1, 54, 171, 172, 82, 173, 174, 23, 18, 31, 6],\n",
              " [1, 54, 171, 172, 82, 173, 174, 23, 18, 31, 6, 27],\n",
              " [13, 10],\n",
              " [13, 10, 42],\n",
              " [13, 10, 42, 83],\n",
              " [13, 10, 42, 83, 84],\n",
              " [13, 10, 42, 83, 84, 2],\n",
              " [13, 10, 42, 83, 84, 2, 175],\n",
              " [13, 10, 42, 83, 84, 2, 175, 6],\n",
              " [13, 10, 42, 83, 84, 2, 175, 6, 176],\n",
              " [11, 177],\n",
              " [11, 177, 43],\n",
              " [11, 177, 43, 9],\n",
              " [11, 177, 43, 9, 31],\n",
              " [11, 177, 43, 9, 31, 5],\n",
              " [11, 177, 43, 9, 31, 5, 178],\n",
              " [11, 177, 43, 9, 31, 5, 178, 179],\n",
              " [11, 177, 43, 9, 31, 5, 178, 179, 51],\n",
              " [11, 177, 43, 9, 31, 5, 178, 179, 51, 10],\n",
              " [180, 55],\n",
              " [180, 55, 16],\n",
              " [180, 55, 16, 181],\n",
              " [180, 55, 16, 181, 182],\n",
              " [180, 55, 16, 181, 182, 1],\n",
              " [180, 55, 16, 181, 182, 1, 13],\n",
              " [180, 55, 16, 181, 182, 1, 13, 35],\n",
              " [180, 55, 16, 181, 182, 1, 13, 35, 183],\n",
              " [180, 55, 16, 181, 182, 1, 13, 35, 183, 22],\n",
              " [184, 52],\n",
              " [184, 52, 185],\n",
              " [184, 52, 185, 1],\n",
              " [184, 52, 185, 1, 186],\n",
              " [184, 52, 185, 1, 186, 3],\n",
              " [184, 52, 185, 1, 186, 3, 85],\n",
              " [184, 52, 185, 1, 186, 3, 85, 2],\n",
              " [184, 52, 185, 1, 186, 3, 85, 2, 187],\n",
              " [184, 52, 185, 1, 186, 3, 85, 2, 187, 188],\n",
              " [8, 56],\n",
              " [8, 56, 189],\n",
              " [8, 56, 189, 82],\n",
              " [8, 56, 189, 82, 86],\n",
              " [8, 56, 189, 82, 86, 1],\n",
              " [8, 56, 189, 82, 86, 1, 13],\n",
              " [8, 56, 189, 82, 86, 1, 13, 190],\n",
              " [8, 56, 189, 82, 86, 1, 13, 190, 191],\n",
              " [8, 56, 189, 82, 86, 1, 13, 190, 191, 192],\n",
              " [87, 88],\n",
              " [87, 88, 11],\n",
              " [87, 88, 11, 8],\n",
              " [87, 88, 11, 8, 56],\n",
              " [87, 88, 11, 8, 56, 57],\n",
              " [87, 88, 11, 8, 56, 57, 193],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1, 13],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1, 13, 5],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1, 13, 5, 194],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1, 13, 5, 194, 51],\n",
              " [87, 88, 11, 8, 56, 57, 193, 1, 13, 5, 194, 51, 10],\n",
              " [87, 195],\n",
              " [87, 195, 11],\n",
              " [87, 195, 11, 8],\n",
              " [87, 195, 11, 8, 56],\n",
              " [87, 195, 11, 8, 56, 57],\n",
              " [87, 195, 11, 8, 56, 57, 196],\n",
              " [87, 195, 11, 8, 56, 57, 196, 197],\n",
              " [87, 195, 11, 8, 56, 57, 196, 197, 198],\n",
              " [87, 195, 11, 8, 56, 57, 196, 197, 198, 2],\n",
              " [87, 195, 11, 8, 56, 57, 196, 197, 198, 2, 1],\n",
              " [87, 195, 11, 8, 56, 57, 196, 197, 198, 2, 1, 13],\n",
              " [74, 199],\n",
              " [74, 199, 16],\n",
              " [74, 199, 16, 200],\n",
              " [74, 199, 16, 200, 201],\n",
              " [74, 199, 16, 200, 201, 18],\n",
              " [74, 199, 16, 200, 201, 18, 31],\n",
              " [74, 199, 16, 200, 201, 18, 31, 6],\n",
              " [74, 199, 16, 200, 201, 18, 31, 6, 12],\n",
              " [74, 199, 16, 200, 201, 18, 31, 6, 12, 27],\n",
              " [1, 202],\n",
              " [1, 202, 203],\n",
              " [1, 202, 203, 204],\n",
              " [1, 202, 203, 204, 1],\n",
              " [1, 202, 203, 204, 1, 73],\n",
              " [1, 202, 203, 204, 1, 73, 205],\n",
              " [1, 202, 203, 204, 1, 73, 205, 6],\n",
              " [1, 202, 203, 204, 1, 73, 205, 6, 1],\n",
              " [1, 202, 203, 204, 1, 73, 205, 6, 1, 17],\n",
              " [1, 202, 203, 204, 1, 73, 205, 6, 1, 17, 12],\n",
              " [206, 207],\n",
              " [206, 207, 208],\n",
              " [206, 207, 208, 1],\n",
              " [206, 207, 208, 1, 209],\n",
              " [206, 207, 208, 1, 209, 2],\n",
              " [206, 207, 208, 1, 209, 2, 210],\n",
              " [206, 207, 208, 1, 209, 2, 210, 211],\n",
              " [206, 207, 208, 1, 209, 2, 210, 211, 23],\n",
              " [212, 58],\n",
              " [212, 58, 15],\n",
              " [212, 58, 15, 30],\n",
              " [212, 58, 15, 30, 31],\n",
              " [212, 58, 15, 30, 31, 6],\n",
              " [212, 58, 15, 30, 31, 6, 59],\n",
              " [212, 58, 15, 30, 31, 6, 59, 213],\n",
              " [212, 58, 15, 30, 31, 6, 59, 213, 19],\n",
              " [212, 58, 15, 30, 31, 6, 59, 213, 19, 44],\n",
              " [214, 58],\n",
              " [214, 58, 215],\n",
              " [214, 58, 215, 216],\n",
              " [214, 58, 215, 216, 1],\n",
              " [214, 58, 215, 216, 1, 217],\n",
              " [214, 58, 215, 216, 1, 217, 218],\n",
              " [214, 58, 215, 216, 1, 217, 218, 17],\n",
              " [214, 58, 215, 216, 1, 217, 218, 17, 12],\n",
              " [214, 58, 215, 216, 1, 217, 218, 17, 12, 219],\n",
              " [214, 58, 215, 216, 1, 217, 218, 17, 12, 219, 220],\n",
              " [214, 58, 215, 216, 1, 217, 218, 17, 12, 219, 220, 50],\n",
              " [221, 55],\n",
              " [221, 55, 222],\n",
              " [221, 55, 222, 223],\n",
              " [221, 55, 222, 223, 53],\n",
              " [221, 55, 222, 223, 53, 89],\n",
              " [221, 55, 222, 223, 53, 89, 19],\n",
              " [14, 4],\n",
              " [14, 4, 72],\n",
              " [14, 4, 72, 224],\n",
              " [14, 4, 72, 224, 225],\n",
              " [14, 4, 72, 224, 225, 3],\n",
              " [14, 4, 72, 224, 225, 3, 226],\n",
              " [14, 4, 72, 224, 225, 3, 226, 227],\n",
              " [46, 228],\n",
              " [46, 228, 11],\n",
              " [46, 228, 11, 229],\n",
              " [46, 228, 11, 229, 230],\n",
              " [46, 228, 11, 229, 230, 20],\n",
              " [46, 228, 11, 229, 230, 20, 231],\n",
              " [46, 228, 11, 229, 230, 20, 231, 1],\n",
              " [46, 228, 11, 229, 230, 20, 231, 1, 232],\n",
              " [46, 228, 11, 229, 230, 20, 231, 1, 232, 17],\n",
              " [46, 228, 11, 229, 230, 20, 231, 1, 232, 17, 12],\n",
              " [1, 76],\n",
              " [1, 76, 2],\n",
              " [1, 76, 2, 47],\n",
              " [1, 76, 2, 47, 233],\n",
              " [1, 76, 2, 47, 233, 40],\n",
              " [1, 76, 2, 47, 233, 40, 59],\n",
              " [1, 76, 2, 47, 233, 40, 59, 234],\n",
              " [1, 76, 2, 47, 233, 40, 59, 234, 10],\n",
              " [1, 76, 2, 47, 233, 40, 59, 234, 10, 22],\n",
              " [10, 15],\n",
              " [10, 15, 30],\n",
              " [10, 15, 30, 235],\n",
              " [10, 15, 30, 235, 236],\n",
              " [10, 15, 30, 235, 236, 237],\n",
              " [10, 15, 30, 235, 236, 237, 5],\n",
              " [10, 15, 30, 235, 236, 237, 5, 85],\n",
              " [10, 15, 30, 235, 236, 237, 5, 85, 238],\n",
              " [10, 15, 30, 235, 236, 237, 5, 85, 238, 239],\n",
              " [75, 90],\n",
              " [75, 90, 240],\n",
              " [75, 90, 240, 241],\n",
              " [75, 90, 240, 241, 1],\n",
              " [75, 90, 240, 241, 1, 13],\n",
              " [75, 90, 240, 241, 1, 13, 9],\n",
              " [75, 90, 240, 241, 1, 13, 9, 242],\n",
              " [75, 90, 240, 241, 1, 13, 9, 242, 243],\n",
              " [244, 8],\n",
              " [244, 8, 91],\n",
              " [244, 8, 91, 1],\n",
              " [244, 8, 91, 1, 25],\n",
              " [244, 8, 91, 1, 25, 2],\n",
              " [244, 8, 91, 1, 25, 2, 60],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32, 245],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32, 245, 16],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32, 245, 16, 246],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32, 245, 16, 246, 92],\n",
              " [244, 8, 91, 1, 25, 2, 60, 32, 245, 16, 246, 92, 247],\n",
              " [93, 84],\n",
              " [93, 84, 15],\n",
              " [93, 84, 15, 248],\n",
              " [93, 84, 15, 248, 60],\n",
              " [93, 84, 15, 248, 60, 32],\n",
              " [93, 84, 15, 248, 60, 32, 4],\n",
              " [93, 84, 15, 248, 60, 32, 4, 5],\n",
              " [93, 84, 15, 248, 60, 32, 4, 5, 94],\n",
              " [93, 84, 15, 248, 60, 32, 4, 5, 94, 249],\n",
              " [29, 4],\n",
              " [29, 4, 250],\n",
              " [29, 4, 250, 59],\n",
              " [29, 4, 250, 59, 251],\n",
              " [29, 4, 250, 59, 251, 3],\n",
              " [29, 4, 250, 59, 251, 3, 70],\n",
              " [29, 4, 250, 59, 251, 3, 70, 5],\n",
              " [29, 4, 250, 59, 251, 3, 70, 5, 252],\n",
              " [29, 4, 250, 59, 251, 3, 70, 5, 252, 253],\n",
              " [22, 254],\n",
              " [22, 254, 95],\n",
              " [22, 254, 95, 15],\n",
              " [22, 254, 95, 15, 255],\n",
              " [22, 254, 95, 15, 255, 1],\n",
              " [22, 254, 95, 15, 255, 1, 10],\n",
              " [22, 254, 95, 15, 255, 1, 10, 43],\n",
              " [89, 19],\n",
              " [89, 19, 256],\n",
              " [89, 19, 256, 30],\n",
              " [89, 19, 256, 30, 257],\n",
              " [89, 19, 256, 30, 257, 3],\n",
              " [89, 19, 256, 30, 257, 3, 258],\n",
              " [89, 19, 256, 30, 257, 3, 258, 259],\n",
              " [37, 2],\n",
              " [37, 2, 17],\n",
              " [37, 2, 17, 12],\n",
              " [37, 2, 17, 12, 27],\n",
              " [37, 2, 17, 12, 27, 96],\n",
              " [37, 2, 17, 12, 27, 96, 260],\n",
              " [37, 2, 17, 12, 27, 96, 260, 261],\n",
              " [37, 2, 17, 12, 27, 96, 260, 261, 3],\n",
              " [37, 2, 17, 12, 27, 96, 260, 261, 3, 97],\n",
              " [36, 262],\n",
              " [36, 262, 263],\n",
              " [36, 262, 263, 264],\n",
              " [36, 262, 263, 264, 265],\n",
              " [36, 262, 263, 264, 265, 266],\n",
              " [36, 262, 263, 264, 265, 266, 1],\n",
              " [36, 262, 263, 264, 265, 266, 1, 17],\n",
              " [36, 262, 263, 264, 265, 266, 1, 17, 12],\n",
              " [36, 262, 263, 264, 265, 266, 1, 17, 12, 5],\n",
              " [36, 262, 263, 264, 265, 266, 1, 17, 12, 5, 61],\n",
              " [46, 267],\n",
              " [46, 267, 4],\n",
              " [46, 267, 4, 38],\n",
              " [46, 267, 4, 38, 98],\n",
              " [46, 267, 4, 38, 98, 64],\n",
              " [46, 267, 4, 38, 98, 64, 20],\n",
              " [46, 267, 4, 38, 98, 64, 20, 99],\n",
              " [46, 267, 4, 38, 98, 64, 20, 99, 268],\n",
              " [46, 267, 4, 38, 98, 64, 20, 99, 268, 37],\n",
              " [17, 12],\n",
              " [17, 12, 27],\n",
              " [17, 12, 27, 9],\n",
              " [17, 12, 27, 9, 11],\n",
              " [17, 12, 27, 9, 11, 50],\n",
              " [17, 12, 27, 9, 11, 50, 269],\n",
              " [17, 12, 27, 9, 11, 50, 269, 270],\n",
              " [17, 12, 27, 9, 11, 50, 269, 270, 271],\n",
              " [17, 12, 27, 9, 11, 50, 269, 270, 271, 44],\n",
              " [54, 5],\n",
              " [54, 5, 54],\n",
              " [54, 5, 54, 4],\n",
              " [54, 5, 54, 4, 15],\n",
              " [54, 5, 54, 4, 15, 272],\n",
              " [54, 5, 54, 4, 15, 272, 62],\n",
              " [54, 5, 54, 4, 15, 272, 62, 16],\n",
              " [54, 5, 54, 4, 15, 272, 62, 16, 273],\n",
              " [54, 5, 54, 4, 15, 272, 62, 16, 273, 3],\n",
              " [54, 5, 54, 4, 15, 272, 62, 16, 273, 3, 274],\n",
              " [275, 276],\n",
              " [275, 276, 100],\n",
              " [275, 276, 100, 1],\n",
              " [275, 276, 100, 1, 277],\n",
              " [275, 276, 100, 1, 277, 2],\n",
              " [275, 276, 100, 1, 277, 2, 4],\n",
              " [275, 276, 100, 1, 277, 2, 4, 5],\n",
              " [275, 276, 100, 1, 277, 2, 4, 5, 278],\n",
              " [275, 276, 100, 1, 277, 2, 4, 5, 278, 24],\n",
              " [275, 276, 100, 1, 277, 2, 4, 5, 278, 24, 49],\n",
              " [275, 276, 100, 1, 277, 2, 4, 5, 278, 24, 49, 23],\n",
              " [101, 4],\n",
              " [101, 4, 279],\n",
              " [101, 4, 279, 280],\n",
              " [101, 4, 279, 280, 281],\n",
              " [101, 4, 279, 280, 281, 20],\n",
              " [101, 4, 279, 280, 281, 20, 45],\n",
              " [101, 4, 279, 280, 281, 20, 45, 14],\n",
              " [101, 4, 279, 280, 281, 20, 45, 14, 62],\n",
              " [282, 3],\n",
              " [282, 3, 283],\n",
              " [282, 3, 283, 18],\n",
              " [282, 3, 283, 18, 284],\n",
              " [282, 3, 283, 18, 284, 2],\n",
              " [282, 3, 283, 18, 284, 2, 67],\n",
              " [282, 3, 283, 18, 284, 2, 67, 101],\n",
              " [282, 3, 283, 18, 284, 2, 67, 101, 78],\n",
              " [285, 4],\n",
              " [285, 4, 18],\n",
              " [285, 4, 18, 32],\n",
              " [285, 4, 18, 32, 24],\n",
              " [285, 4, 18, 32, 24, 286],\n",
              " [285, 4, 18, 32, 24, 286, 2],\n",
              " [285, 4, 18, 32, 24, 286, 2, 23],\n",
              " [285, 4, 18, 32, 24, 286, 2, 23, 35],\n",
              " [285, 4, 18, 32, 24, 286, 2, 23, 35, 29],\n",
              " [285, 4, 18, 32, 24, 286, 2, 23, 35, 29, 287],\n",
              " [60, 32],\n",
              " [60, 32, 4],\n",
              " [60, 32, 4, 15],\n",
              " [60, 32, 4, 15, 30],\n",
              " [60, 32, 4, 15, 30, 93],\n",
              " [60, 32, 4, 15, 30, 93, 288],\n",
              " [60, 32, 4, 15, 30, 93, 288, 6],\n",
              " [60, 32, 4, 15, 30, 93, 288, 6, 94],\n",
              " [60, 32, 4, 15, 30, 93, 288, 6, 94, 289],\n",
              " [60, 32, 4, 15, 30, 93, 288, 6, 94, 289, 62],\n",
              " [45, 14],\n",
              " [45, 14, 102],\n",
              " [45, 14, 102, 9],\n",
              " [45, 14, 102, 9, 290],\n",
              " [45, 14, 102, 9, 290, 5],\n",
              " [45, 14, 102, 9, 290, 5, 291],\n",
              " [45, 14, 102, 9, 290, 5, 291, 7],\n",
              " [45, 14, 102, 9, 290, 5, 291, 7, 21],\n",
              " [68, 292],\n",
              " [68, 292, 21],\n",
              " [68, 292, 21, 293],\n",
              " [68, 292, 21, 293, 294],\n",
              " [68, 292, 21, 293, 294, 35],\n",
              " [68, 292, 21, 293, 294, 35, 14],\n",
              " [68, 292, 21, 293, 294, 35, 14, 4],\n",
              " [295, 296],\n",
              " [295, 296, 48],\n",
              " [295, 296, 48, 297],\n",
              " [295, 296, 48, 297, 24],\n",
              " [295, 296, 48, 297, 24, 17],\n",
              " [295, 296, 48, 297, 24, 17, 12],\n",
              " [295, 296, 48, 297, 24, 17, 12, 27],\n",
              " [295, 296, 48, 297, 24, 17, 12, 27, 4],\n",
              " [7, 298],\n",
              " [7, 298, 58],\n",
              " [7, 298, 58, 299],\n",
              " [7, 298, 58, 299, 25],\n",
              " [7, 298, 58, 299, 25, 14],\n",
              " [7, 298, 58, 299, 25, 14, 4],\n",
              " [7, 298, 58, 299, 25, 14, 4, 6],\n",
              " [7, 298, 58, 299, 25, 14, 4, 6, 300],\n",
              " [7, 298, 58, 299, 25, 14, 4, 6, 300, 102],\n",
              " [301, 97],\n",
              " [301, 97, 16],\n",
              " [301, 97, 16, 302],\n",
              " [301, 97, 16, 302, 3],\n",
              " [301, 97, 16, 302, 3, 303],\n",
              " [301, 97, 16, 302, 3, 303, 25],\n",
              " [301, 97, 16, 302, 3, 303, 25, 36],\n",
              " [301, 97, 16, 302, 3, 303, 25, 36, 14],\n",
              " [301, 97, 16, 302, 3, 303, 25, 36, 14, 304],\n",
              " [77, 305],\n",
              " [77, 305, 14],\n",
              " [77, 305, 14, 4],\n",
              " [77, 305, 14, 4, 5],\n",
              " [77, 305, 14, 4, 5, 306],\n",
              " [77, 305, 14, 4, 5, 306, 103],\n",
              " [77, 305, 14, 4, 5, 306, 103, 16],\n",
              " [77, 305, 14, 4, 5, 306, 103, 16, 307],\n",
              " [308, 309],\n",
              " [308, 309, 25],\n",
              " [308, 309, 25, 7],\n",
              " [308, 309, 25, 7, 5],\n",
              " [308, 309, 25, 7, 5, 104],\n",
              " [308, 309, 25, 7, 5, 104, 310],\n",
              " [308, 309, 25, 7, 5, 104, 310, 8],\n",
              " [308, 309, 25, 7, 5, 104, 310, 8, 311],\n",
              " [312, 37],\n",
              " [312, 37, 2],\n",
              " [312, 37, 2, 7],\n",
              " [312, 37, 2, 7, 96],\n",
              " [312, 37, 2, 7, 96, 313],\n",
              " [312, 37, 2, 7, 96, 313, 314],\n",
              " [312, 37, 2, 7, 96, 313, 314, 315],\n",
              " [312, 37, 2, 7, 96, 313, 314, 315, 3],\n",
              " [312, 37, 2, 7, 96, 313, 314, 315, 3, 316],\n",
              " [317, 318],\n",
              " [317, 318, 25],\n",
              " [317, 318, 25, 36],\n",
              " [317, 318, 25, 36, 4],\n",
              " [317, 318, 25, 36, 4, 6],\n",
              " [317, 318, 25, 36, 4, 6, 319],\n",
              " [317, 318, 25, 36, 4, 6, 319, 320],\n",
              " [317, 318, 25, 36, 4, 6, 319, 320, 3],\n",
              " [317, 318, 25, 36, 4, 6, 319, 320, 3, 321],\n",
              " [322, 63],\n",
              " [322, 63, 21],\n",
              " [322, 63, 21, 48],\n",
              " [322, 63, 21, 48, 24],\n",
              " [322, 63, 21, 48, 24, 34],\n",
              " [322, 63, 21, 48, 24, 34, 8],\n",
              " [322, 63, 21, 48, 24, 34, 8, 6],\n",
              " [322, 63, 21, 48, 24, 34, 8, 6, 323],\n",
              " [322, 63, 21, 48, 24, 34, 8, 6, 323, 324],\n",
              " [325, 326],\n",
              " [325, 326, 105],\n",
              " [325, 326, 105, 25],\n",
              " [325, 326, 105, 25, 14],\n",
              " [325, 326, 105, 25, 14, 4],\n",
              " [325, 326, 105, 25, 14, 4, 6],\n",
              " [325, 326, 105, 25, 14, 4, 6, 327],\n",
              " [325, 326, 105, 25, 14, 4, 6, 327, 328],\n",
              " [329, 21],\n",
              " [329, 21, 25],\n",
              " [329, 21, 25, 7],\n",
              " [329, 21, 25, 7, 5],\n",
              " [329, 21, 25, 7, 5, 330],\n",
              " [329, 21, 25, 7, 5, 330, 331],\n",
              " [329, 21, 25, 7, 5, 330, 331, 332],\n",
              " [329, 21, 25, 7, 5, 330, 331, 332, 24],\n",
              " [329, 21, 25, 7, 5, 330, 331, 332, 24, 98],\n",
              " [329, 21, 25, 7, 5, 330, 331, 332, 24, 98, 333],\n",
              " [1, 334],\n",
              " [1, 334, 2],\n",
              " [1, 334, 2, 7],\n",
              " [1, 334, 2, 7, 42],\n",
              " [1, 334, 2, 7, 42, 83],\n",
              " [1, 334, 2, 7, 42, 83, 335],\n",
              " [1, 334, 2, 7, 42, 83, 335, 2],\n",
              " [1, 334, 2, 7, 42, 83, 335, 2, 336],\n",
              " [1, 334, 2, 7, 42, 83, 335, 2, 336, 3],\n",
              " [1, 334, 2, 7, 42, 83, 335, 2, 336, 3, 106],\n",
              " [107, 7],\n",
              " [107, 7, 108],\n",
              " [107, 7, 108, 81],\n",
              " [107, 7, 108, 81, 4],\n",
              " [107, 7, 108, 81, 4, 337],\n",
              " [107, 7, 108, 81, 4, 337, 338],\n",
              " [107, 7, 108, 81, 4, 337, 338, 339],\n",
              " [107, 7, 108, 81, 4, 337, 338, 339, 340],\n",
              " [106, 20],\n",
              " [106, 20, 10],\n",
              " [106, 20, 10, 22],\n",
              " [106, 20, 10, 22, 15],\n",
              " [106, 20, 10, 22, 15, 341],\n",
              " [106, 20, 10, 22, 15, 341, 5],\n",
              " [106, 20, 10, 22, 15, 341, 5, 342],\n",
              " [106, 20, 10, 22, 15, 341, 5, 342, 92],\n",
              " [106, 20, 10, 22, 15, 341, 5, 342, 92, 343],\n",
              " [106, 20, 10, 22, 15, 341, 5, 342, 92, 343, 47],\n",
              " [344, 20],\n",
              " [344, 20, 7],\n",
              " [344, 20, 7, 90],\n",
              " [344, 20, 7, 90, 345],\n",
              " [344, 20, 7, 90, 345, 346],\n",
              " [344, 20, 7, 90, 345, 346, 347],\n",
              " [344, 20, 7, 90, 345, 346, 347, 109],\n",
              " [344, 20, 7, 90, 345, 346, 347, 109, 3],\n",
              " [344, 20, 7, 90, 345, 346, 347, 109, 3, 348],\n",
              " [349, 7],\n",
              " [349, 7, 9],\n",
              " [349, 7, 9, 11],\n",
              " [349, 7, 9, 11, 350],\n",
              " [349, 7, 9, 11, 350, 351],\n",
              " [349, 7, 9, 11, 350, 351, 2],\n",
              " [349, 7, 9, 11, 350, 351, 2, 110],\n",
              " [349, 7, 9, 11, 350, 351, 2, 110, 20],\n",
              " [349, 7, 9, 11, 350, 351, 2, 110, 20, 34],\n",
              " [349, 7, 9, 11, 350, 351, 2, 110, 20, 34, 8],\n",
              " [1, 352],\n",
              " [1, 352, 2],\n",
              " [1, 352, 2, 4],\n",
              " [1, 352, 2, 4, 91],\n",
              " [1, 352, 2, 4, 91, 109],\n",
              " [1, 352, 2, 4, 91, 109, 5],\n",
              " [1, 352, 2, 4, 91, 109, 5, 353],\n",
              " [1, 352, 2, 4, 91, 109, 5, 353, 354],\n",
              " [355, 356],\n",
              " [355, 356, 357],\n",
              " [355, 356, 357, 358],\n",
              " [355, 356, 357, 358, 7],\n",
              " [355, 356, 357, 358, 7, 108],\n",
              " [355, 356, 357, 358, 7, 108, 359],\n",
              " [355, 356, 357, 358, 7, 108, 359, 5],\n",
              " [355, 356, 357, 358, 7, 108, 359, 5, 360],\n",
              " [361, 16],\n",
              " [361, 16, 362],\n",
              " [361, 16, 362, 3],\n",
              " [361, 16, 362, 3, 363],\n",
              " [361, 16, 362, 3, 363, 18],\n",
              " [361, 16, 362, 3, 363, 18, 364],\n",
              " [361, 16, 362, 3, 363, 18, 364, 31],\n",
              " [361, 16, 362, 3, 363, 18, 364, 31, 6],\n",
              " [361, 16, 362, 3, 363, 18, 364, 31, 6, 41],\n",
              " [361, 16, 362, 3, 363, 18, 364, 31, 6, 41, 8],\n",
              " [365, 366],\n",
              " [365, 366, 11],\n",
              " [365, 366, 11, 88],\n",
              " [365, 366, 11, 88, 367],\n",
              " [365, 366, 11, 88, 367, 368],\n",
              " [365, 366, 11, 88, 367, 368, 6],\n",
              " [365, 366, 11, 88, 367, 368, 6, 80],\n",
              " [365, 366, 11, 88, 367, 368, 6, 80, 28],\n",
              " [365, 366, 11, 88, 367, 368, 6, 80, 28, 26],\n",
              " [365, 366, 11, 88, 367, 368, 6, 80, 28, 26, 86],\n",
              " [13, 369],\n",
              " [13, 369, 370],\n",
              " [13, 369, 370, 111],\n",
              " [13, 369, 370, 111, 32],\n",
              " [13, 369, 370, 111, 32, 4],\n",
              " [13, 369, 370, 111, 32, 4, 5],\n",
              " [13, 369, 370, 111, 32, 4, 5, 99],\n",
              " [13, 369, 370, 111, 32, 4, 5, 99, 61],\n",
              " [371, 372],\n",
              " [371, 372, 373],\n",
              " [371, 372, 373, 2],\n",
              " [371, 372, 373, 2, 7],\n",
              " [371, 372, 373, 2, 7, 4],\n",
              " [371, 372, 373, 2, 7, 4, 53],\n",
              " [371, 372, 373, 2, 7, 4, 53, 37],\n",
              " [371, 372, 373, 2, 7, 4, 53, 37, 3],\n",
              " [371, 372, 373, 2, 7, 4, 53, 37, 3, 374],\n",
              " [375, 9],\n",
              " [375, 9, 49],\n",
              " [375, 9, 49, 376],\n",
              " [375, 9, 49, 376, 111],\n",
              " [375, 9, 49, 376, 111, 4],\n",
              " [375, 9, 49, 376, 111, 4, 5],\n",
              " [375, 9, 49, 376, 111, 4, 5, 377],\n",
              " [375, 9, 49, 376, 111, 4, 5, 377, 2],\n",
              " [375, 9, 49, 376, 111, 4, 5, 377, 2, 61],\n",
              " [378, 105],\n",
              " [378, 105, 104],\n",
              " [378, 105, 104, 379],\n",
              " [378, 105, 104, 379, 6],\n",
              " [378, 105, 104, 379, 6, 10],\n",
              " [378, 105, 104, 379, 6, 10, 3],\n",
              " [378, 105, 104, 379, 6, 10, 3, 380],\n",
              " [378, 105, 104, 379, 6, 10, 3, 380, 7],\n",
              " [378, 105, 104, 379, 6, 10, 3, 380, 7, 4],\n",
              " [381, 382],\n",
              " [381, 382, 383],\n",
              " [381, 382, 383, 7],\n",
              " [381, 382, 383, 7, 4],\n",
              " [381, 382, 383, 7, 4, 5],\n",
              " [381, 382, 383, 7, 4, 5, 384],\n",
              " [381, 382, 383, 7, 4, 5, 384, 385],\n",
              " [381, 382, 383, 7, 4, 5, 384, 385, 386],\n",
              " [381, 382, 383, 7, 4, 5, 384, 385, 386, 387],\n",
              " [13, 388],\n",
              " [13, 388, 55],\n",
              " [13, 388, 55, 389],\n",
              " [13, 388, 55, 389, 1],\n",
              " [13, 388, 55, 389, 1, 52],\n",
              " [13, 388, 55, 389, 1, 52, 2],\n",
              " [13, 388, 55, 389, 1, 52, 2, 29],\n",
              " [13, 388, 55, 389, 1, 52, 2, 29, 28],\n",
              " [13, 388, 55, 389, 1, 52, 2, 29, 28, 26],\n",
              " [390, 3],\n",
              " [390, 3, 391],\n",
              " [390, 3, 391, 18],\n",
              " [390, 3, 391, 18, 392],\n",
              " [390, 3, 391, 18, 392, 95],\n",
              " [390, 3, 391, 18, 392, 95, 6],\n",
              " [390, 3, 391, 18, 392, 95, 6, 112],\n",
              " [390, 3, 391, 18, 392, 95, 6, 112, 4],\n",
              " [393, 112],\n",
              " [393, 112, 7],\n",
              " [393, 112, 7, 9],\n",
              " [393, 112, 7, 9, 79],\n",
              " [393, 112, 7, 9, 79, 394],\n",
              " [393, 112, 7, 9, 79, 394, 110],\n",
              " [393, 112, 7, 9, 79, 394, 110, 395],\n",
              " [393, 112, 7, 9, 79, 394, 110, 395, 6],\n",
              " [393, 112, 7, 9, 79, 394, 110, 395, 6, 396],\n",
              " [33, 7],\n",
              " [33, 7, 21],\n",
              " [33, 7, 21, 57],\n",
              " [33, 7, 21, 57, 397],\n",
              " [33, 7, 21, 57, 397, 398],\n",
              " [33, 7, 21, 57, 397, 398, 40],\n",
              " [33, 7, 21, 57, 397, 398, 40, 399],\n",
              " [33, 7, 21, 57, 397, 398, 40, 399, 8],\n",
              " [103, 3],\n",
              " [103, 3, 7],\n",
              " [103, 3, 7, 400],\n",
              " [103, 3, 7, 400, 15],\n",
              " [103, 3, 7, 400, 15, 100],\n",
              " [103, 3, 7, 400, 15, 100, 401],\n",
              " [103, 3, 7, 400, 15, 100, 401, 3],\n",
              " [103, 3, 7, 400, 15, 100, 401, 3, 402],\n",
              " [403, 404],\n",
              " [403, 404, 7],\n",
              " [403, 404, 7, 9],\n",
              " [403, 404, 7, 9, 405],\n",
              " [403, 404, 7, 9, 405, 6],\n",
              " [403, 404, 7, 9, 405, 6, 406],\n",
              " [403, 404, 7, 9, 405, 6, 406, 1],\n",
              " [403, 404, 7, 9, 405, 6, 406, 1, 33],\n",
              " [403, 404, 7, 9, 405, 6, 406, 1, 33, 407],\n",
              " [408, 409],\n",
              " [408, 409, 18],\n",
              " [408, 409, 18, 410],\n",
              " [408, 409, 18, 410, 6],\n",
              " [408, 409, 18, 410, 6, 411],\n",
              " [408, 409, 18, 410, 6, 411, 3],\n",
              " [408, 409, 18, 410, 6, 411, 3, 412],\n",
              " [408, 409, 18, 410, 6, 411, 3, 412, 7],\n",
              " [408, 409, 18, 410, 6, 411, 3, 412, 7, 413],\n",
              " [1, 33],\n",
              " [1, 33, 2],\n",
              " [1, 33, 2, 7],\n",
              " [1, 33, 2, 7, 414],\n",
              " [1, 33, 2, 7, 414, 24],\n",
              " [1, 33, 2, 7, 414, 24, 107],\n",
              " [1, 33, 2, 7, 414, 24, 107, 415],\n",
              " [1, 33, 2, 7, 414, 24, 107, 415, 3],\n",
              " [1, 33, 2, 7, 414, 24, 107, 415, 3, 416]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "input_sequannce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vs4XN-dR2PEh"
      },
      "outputs": [],
      "source": [
        "max_len=max([len(x) for x in  input_sequannce])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "090TnW8e2fcZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequance=pad_sequences(input_sequannce,maxlen = max_len,padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJCb_f822iuk",
        "outputId": "a63deccf-e8cd-40fb-99a7-bac67ac4ab39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0, 113,  63],\n",
              "       [  0,   0,   0, ..., 113,  63,   9],\n",
              "       [  0,   0,   0, ...,  63,   9, 114],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  24, 107, 415],\n",
              "       [  0,   0,   0, ..., 107, 415,   3],\n",
              "       [  0,   0,   0, ..., 415,   3, 416]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "padded_input_sequance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uoj2akMI2lh2"
      },
      "outputs": [],
      "source": [
        "x=padded_input_sequance[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fnS9KGeB2pJ5"
      },
      "outputs": [],
      "source": [
        "y=padded_input_sequance[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MdtQhzN2s37",
        "outputId": "b15affeb-a6aa-4ded-c177-57a4b9c06e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0   0 113]\n",
            " [  0   0   0 ...   0 113  63]\n",
            " [  0   0   0 ... 113  63   9]\n",
            " ...\n",
            " [  0   0   0 ... 414  24 107]\n",
            " [  0   0   0 ...  24 107 415]\n",
            " [  0   0   0 ... 107 415   3]]\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NnHjKnvR2vZX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGTBXpLG2xuY",
        "outputId": "fbb62f74-17f0-49e8-fca8-a8d362270dcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eSulKtvZ21Qs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h9KFRzy424Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab820ef-380f-4195-ab7c-a76028559d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(417,100,input_length=56))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(417,activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WIskrtk827Cq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "QoQTOZed2-rP",
        "outputId": "841eb632-0495-4d6b-c941-c56bdd25da98"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " embedding (\u001b[38;5;33mEmbedding\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m100\u001b[0m)                \u001b[38;5;34m41,700\u001b[0m \n",
              "\n",
              " lstm (\u001b[38;5;33mLSTM\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                   \u001b[38;5;34m150,600\u001b[0m \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m417\u001b[0m)                    \u001b[38;5;34m62,967\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">41,700</span> \n",
              "\n",
              " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">417</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">62,967</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m255,267\u001b[0m (997.14 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">255,267</span> (997.14 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m255,267\u001b[0m (997.14 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">255,267</span> (997.14 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.build(input_shape=(None, 56))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDC2i1kr3DuF",
        "outputId": "e5bae6a7-adf2-47dd-c402-0eeb222b3348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0, 113],\n",
              "       [  0,   0,   0, ...,   0, 113,  63],\n",
              "       [  0,   0,   0, ..., 113,  63,   9],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 414,  24, 107],\n",
              "       [  0,   0,   0, ...,  24, 107, 415],\n",
              "       [  0,   0,   0, ..., 107, 415,   3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bb7uMofAa8Y",
        "outputId": "dae7c3ba-2c0a-4e55-9b63-d5b9262bf050"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-owE9k8AOsu",
        "outputId": "c04ad1b1-b08f-4c9a-b13e-b0e7b77267b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.0129 - loss: 5.9946\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0492 - loss: 5.5213\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0438 - loss: 5.3339\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0437 - loss: 5.3633\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0348 - loss: 5.2349\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0526 - loss: 5.1209\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0536 - loss: 5.0232\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0661 - loss: 4.9028\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0560 - loss: 4.9424\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0676 - loss: 4.8360\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0815 - loss: 4.6306\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1000 - loss: 4.4541\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1327 - loss: 4.3524\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1403 - loss: 4.1574\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1855 - loss: 3.9995\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2153 - loss: 3.8540\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2487 - loss: 3.5919\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2810 - loss: 3.4161\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3313 - loss: 3.2110\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3150 - loss: 3.1827\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3799 - loss: 2.9198\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4308 - loss: 2.7593\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4732 - loss: 2.5963\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5160 - loss: 2.4599\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5794 - loss: 2.3229\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5999 - loss: 2.1199\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6530 - loss: 2.0397\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7018 - loss: 1.8587\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7318 - loss: 1.7540\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7557 - loss: 1.7048\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7926 - loss: 1.5533\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8230 - loss: 1.4074\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8197 - loss: 1.3517\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8247 - loss: 1.2690\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8667 - loss: 1.1473\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8768 - loss: 1.0699\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8835 - loss: 0.9974\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8940 - loss: 0.9556\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9179 - loss: 0.8719\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9177 - loss: 0.8564\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9295 - loss: 0.7880\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9440 - loss: 0.7098\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9515 - loss: 0.6270\n",
            "Epoch 44/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9493 - loss: 0.6088\n",
            "Epoch 45/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9475 - loss: 0.5904\n",
            "Epoch 46/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.5816\n",
            "Epoch 47/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9622 - loss: 0.5028\n",
            "Epoch 48/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9668 - loss: 0.4650\n",
            "Epoch 49/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9691 - loss: 0.4425\n",
            "Epoch 50/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9617 - loss: 0.4378\n",
            "Epoch 51/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9677 - loss: 0.4044\n",
            "Epoch 52/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9700 - loss: 0.3644\n",
            "Epoch 53/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.3478\n",
            "Epoch 54/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9824 - loss: 0.3160\n",
            "Epoch 55/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9709 - loss: 0.3224\n",
            "Epoch 56/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9767 - loss: 0.2987\n",
            "Epoch 57/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9753 - loss: 0.2879\n",
            "Epoch 58/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9800 - loss: 0.2710\n",
            "Epoch 59/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9707 - loss: 0.2428\n",
            "Epoch 60/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9756 - loss: 0.2423\n",
            "Epoch 61/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.2264\n",
            "Epoch 62/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9826 - loss: 0.2160\n",
            "Epoch 63/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9745 - loss: 0.2129\n",
            "Epoch 64/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9805 - loss: 0.1848\n",
            "Epoch 65/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9769 - loss: 0.1889\n",
            "Epoch 66/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9845 - loss: 0.1871\n",
            "Epoch 67/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9896 - loss: 0.1615\n",
            "Epoch 68/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.1621\n",
            "Epoch 69/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9808 - loss: 0.1581\n",
            "Epoch 70/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9761 - loss: 0.1590\n",
            "Epoch 71/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.1661\n",
            "Epoch 72/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9777 - loss: 0.1439\n",
            "Epoch 73/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9793 - loss: 0.1445\n",
            "Epoch 74/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9807 - loss: 0.1309\n",
            "Epoch 75/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9755 - loss: 0.1367\n",
            "Epoch 76/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9728 - loss: 0.1315\n",
            "Epoch 77/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9793 - loss: 0.1199\n",
            "Epoch 78/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9742 - loss: 0.1243\n",
            "Epoch 79/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9783 - loss: 0.1097\n",
            "Epoch 80/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9809 - loss: 0.1043\n",
            "Epoch 81/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9797 - loss: 0.1029\n",
            "Epoch 82/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9827 - loss: 0.1104\n",
            "Epoch 83/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9668 - loss: 0.1205\n",
            "Epoch 84/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.1001\n",
            "Epoch 85/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0906\n",
            "Epoch 86/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0926\n",
            "Epoch 87/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9805 - loss: 0.0898\n",
            "Epoch 88/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9805 - loss: 0.1026\n",
            "Epoch 89/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.0902\n",
            "Epoch 90/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9793 - loss: 0.0950\n",
            "Epoch 91/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9757 - loss: 0.0916\n",
            "Epoch 92/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9773 - loss: 0.0902\n",
            "Epoch 93/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9752 - loss: 0.0931\n",
            "Epoch 94/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9741 - loss: 0.0897\n",
            "Epoch 95/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.0760\n",
            "Epoch 96/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9871 - loss: 0.0730\n",
            "Epoch 97/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9804 - loss: 0.0744\n",
            "Epoch 98/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9807 - loss: 0.0773\n",
            "Epoch 99/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9797 - loss: 0.0776\n",
            "Epoch 100/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9764 - loss: 0.0765\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b77e292e420>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.fit(x,y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PBqh3ujFKlGU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xywE5_yPBLAZ",
        "outputId": "429f37f2-1e05-4cf2-d3ff-c9c87cb3f3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "rate\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "rate\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "rate\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "rate\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "rate\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "for i in range(5):\n",
        " text = \"learning\"\n",
        " tokentext = tokenizer.texts_to_sequences([text])[0]\n",
        " padded_token_text = pad_sequences([tokentext], maxlen=max_len, padding='pre')\n",
        " pos = np.argmax(model.predict(padded_token_text))\n",
        " time.sleep(2)\n",
        " for word , index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text=text + \"the\" + word\n",
        "      print(word)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}